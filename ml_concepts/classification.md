## ðŸ“Œ Classification Algorithms

| Model                  | Description                                           | Pros                                               | Cons                                              | Example Code                                    | Notes                             |
|------------------------|-------------------------------------------------------|----------------------------------------------------|---------------------------------------------------|--------------------------------------------------|-----------------------------------|
| Naive Bayes            | Probabilistic model for discrete features            | Simple, fast, great for text                       | Strong independence assumption                    | `MultinomialNB()` (sklearn)                      | Use with bag-of-words or TF-IDF   |
| Logistic Regression    | Linear model for classification                      | Interpretable, fast                                | Poor with non-linear boundaries                   | `LogisticRegression()` (sklearn)                | Add regularization (L1/L2)        |
| Decision Tree          | Tree structure based on feature splits               | Interpretable, no scaling needed                   | Easily overfits                                   | `DecisionTreeClassifier()` (sklearn)            | Basis for ensembles               |
| Random Forest          | Ensemble of decision trees                           | Robust, handles non-linearities                    | Less interpretable                                | `RandomForestClassifier()` (sklearn)            | Good default model                |
| ExtraTreesClassifier   | More randomized forest                               | Faster than RF, good for high-dim data             | May overfit small data                            | `ExtraTreesClassifier()` (sklearn)              | Try when RF is slow               |
| XGBoost                | Extreme gradient boosting                            | Accurate, fast, handles missing values             | Requires careful tuning                           | `XGBClassifier()` (xgboost)                     | Great on tabular data             |
| LightGBM               | Efficient gradient boosting                          | Extremely fast, great on large datasets            | Requires numerical features                       | `LGBMClassifier()` (lightgbm)                   | GPU support                       |
| CatBoost               | Handles categorical data natively                    | Great performance, minimal tuning                  | Slower training                                   | `CatBoostClassifier()` (catboost)               | No label encoding needed          |
| SVC (SVM)              | Max-margin classifier with kernel trick              | Effective in high-dimensions                       | Slow on large data                                | `SVC(kernel='rbf')` (sklearn)                   | Use `LinearSVC` for large data    |
| SGDClassifier          | Online learning linear model                         | Very fast, handles streaming                       | Sensitive to scale                                | `SGDClassifier()` (sklearn)                     | Add regularization                |
| KNN Classifier         | Lazy classifier based on neighbors                   | No training, interpretable                         | Slow prediction                                   | `KNeighborsClassifier()` (sklearn)              | Normalize features                |
| MLPClassifier          | Neural network (feed-forward)                        | Captures complex patterns                          | Needs more data and tuning                        | `MLPClassifier(hidden_layer_sizes=(100,))`      | Sensitive to scaling              |
| VotingClassifier       | Combines multiple classifiers                        | Simple ensemble                                    | Needs similar scale models                        | `VotingClassifier(estimators=[...])` (sklearn)  | Hard or soft voting               |
| StackingClassifier     | Meta-model on top of base classifiers                | Often best performance                             | More complexity                                   | `StackingClassifier(estimators=[...])` (sklearn)| Can overfit small datasets        |
