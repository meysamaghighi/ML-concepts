## üß† What is Deep Learning?

**Deep Learning (DL)** is a subfield of Machine Learning that uses multi-layered neural networks to learn representations of data. Unlike traditional ML, DL often removes the need for manual feature engineering. It learns hierarchical representations directly from raw data (e.g., pixels, sound, text).

The "deep" refers to the **depth** (i.e., number of layers) in the network.

---

## üèóÔ∏è Core Concepts and Building Blocks

### 1. **Feed-Forward Neural Networks (FNNs)**
The simplest type of neural network. Information moves in one direction‚Äîfrom input to output‚Äîthrough layers of neurons.

- **Structure**: Input layer ‚Üí Hidden layer(s) ‚Üí Output layer
- Each neuron computes:  
```math
  y = \sigma(Wx + b)
```
  where $\sigma$ is an activation function (more below).

### 2. **Activation Functions**
They introduce non-linearity to the network, allowing it to learn complex patterns.

- **Sigmoid**:  
```math
  \sigma(x) = \frac{1}{1 + e^{-x}}
```
  Suffered from vanishing gradients.

- **ReLU (Rectified Linear Unit)**:  
```math
  f(x) = \max(0, x)
```
  Most common now, fast convergence.

- **Others**: Tanh, Leaky ReLU, ELU, GELU (used in Transformers)

### 3. **Backpropagation**
The algorithm for training neural networks.

- Uses **chain rule** to compute gradients of the loss function w.r.t each weight.
- Combined with **gradient descent** (or its variants like Adam) to update weights.

### 4. **Vanishing Gradient Problem**
Occurs when gradients shrink too much during backpropagation, especially in deep networks or RNNs. This leads to very slow learning in earlier layers.

---

## üß± Specialized Architectures

### 5. **Convolutional Neural Networks (CNNs)**
Designed for image and spatial data.

- Use **convolutional layers** instead of fully connected ones to capture **local patterns** (like edges).
- Components:  
  - **Convolution**: Feature extraction with small kernels  
  - **Pooling**: Downsampling  
  - **Fully connected**: Final classification

> Example: Image classification (e.g., cats vs dogs)

### 6. **Recurrent Neural Networks (RNNs)**
Designed for sequential data (e.g., time series, text).

- Maintains a **hidden state** that gets updated at each timestep:
```math
  h_t = f(Wx_t + Uh_{t-1} + b)
```
- Struggles with long sequences due to vanishing gradients.

### 7. **Long Short-Term Memory (LSTM)**
A type of RNN that handles long-term dependencies.

- Introduces **gates**: input, forget, and output gates that control information flow.
- Better at remembering context over long sequences.

---

## üß™ Regularization and Training Tricks

### 8. **Dropout**
Regularization technique to prevent overfitting.

- Randomly "drops out" (sets to zero) neurons during training.
- Forces the network to not rely on specific neurons, promoting redundancy and robustness.

---

## üßô‚Äç‚ôÇÔ∏è Generative and Attention-Based Models

### 9. **GANs (Generative Adversarial Networks)**
Used for data generation (e.g., images, audio).

- **Two networks**:  
  - **Generator** tries to create realistic data.  
  - **Discriminator** tries to distinguish real from fake.

- They play a **minimax game**:
```math
  \min_G \max_D V(D, G)
```

- Outcome: Generator learns to create convincing fake data.

### 10. **Attention Mechanisms**
Introduced to help models **focus on relevant parts of the input** sequence.

- Scores each input token's relevance to the output being generated.
- Formula (simplified):
```math
  \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
```
- Used in machine translation, summarization, and more.

---

## ‚ö° Stepping into the Modern Era: Transformers (Just a Teaser)

- **Transformers** are based entirely on attention‚Äîno RNNs or CNNs.
- Excel at NLP and now vision (e.g., GPT, BERT, ViT).
- Solve the vanishing gradient issue and enable parallelism.

We can go deeper into them later if you're interested.

---

## üéØ Summary Table

| Concept                 | Use Case / Function                    | Challenge Addressed                     |
|------------------------|----------------------------------------|------------------------------------------|
| Feed-forward NN        | General modeling                       | Basic architecture                       |
| Activation functions   | Non-linearity                          | Enables learning complex patterns        |
| Backpropagation        | Training                               | Efficient gradient calculation           |
| CNN                    | Image tasks                            | Spatial invariance                       |
| RNN                    | Sequences (text, time)                 | Temporal modeling                        |
| LSTM                   | Long sequences                         | Long-term dependencies                   |
| Dropout                | Regularization                         | Overfitting                              |
| GANs                   | Data generation                        | Creating synthetic but realistic data    |
| Attention              | NLP, Seq2Seq, Transformers             | Focus on relevant input features         |
| Vanishing gradient     | Deep networks, RNNs                    | Learning stalls in early layers          |
